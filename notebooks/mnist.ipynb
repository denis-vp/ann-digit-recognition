{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the data set",
   "id": "c8da224990b5bb1c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:39:49.623866Z",
     "start_time": "2024-05-06T20:39:49.564792Z"
    }
   },
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create data loaders",
   "id": "762fa1a0d2dcd78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:39:49.642333Z",
     "start_time": "2024-05-06T20:39:49.632544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ],
   "id": "cfb7eb33a967eb",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modify target labels to represent probabilities",
   "id": "4ff785a89fe4fe26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:39:49.696924Z",
     "start_time": "2024-05-06T20:39:49.679624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_probability(num_labels, label):\n",
    "    return [1 if i == label else 0 for i in range(num_labels)]\n",
    "\n",
    "num_labels = 10"
   ],
   "id": "570134e7f36b637e",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize the model",
   "id": "4d9d9ebcfaacbd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:39:49.725923Z",
     "start_time": "2024-05-06T20:39:49.710960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from neural_network.neural_network import NeuralNetwork\n",
    "from neural_network.functions.loss_functions import CrossEntropy\n",
    "from neural_network.functions.activation_functions import ReLU, Softmax\n",
    "\n",
    "hidden_layer_sizes = [784, 128, 64, 10]\n",
    "hidden_layer_activations = [ReLU(), ReLU(), Softmax()]\n",
    "loss_function =CrossEntropy()\n",
    "\n",
    "model = NeuralNetwork(hidden_layer_sizes, hidden_layer_activations, loss_function)\n",
    "\n",
    "learning_rate = 0.01\n",
    "regularization_rate = 0.01\n",
    "momentum = 0.9"
   ],
   "id": "c6a19f8657278e06",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train the model",
   "id": "10eb101203e3e3c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:39:49.730319Z",
     "start_time": "2024-05-06T20:39:49.726928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict(y_pred):\n",
    "    return np.argmax(y_pred)\n",
    "\n",
    "\n",
    "def my_flatten(data: torch.Tensor) -> list[float]:\n",
    "    return data.flatten().tolist()"
   ],
   "id": "a1eafaf76c752d21",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T21:36:51.020060Z",
     "start_time": "2024-05-06T20:39:49.731401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model.reset()\n",
    "\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f'Epoch: {epoch + 1}/{num_epochs}\\n--------------')\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X = np.array([my_flatten(x) for x in X])\n",
    "        y = np.array([make_probability(num_labels, label) for label in y])\n",
    "        \n",
    "        for x, y in zip(X, y):\n",
    "            y_pred = model.forward(x)\n",
    "            \n",
    "            loss = model.get_loss(y, y_pred)\n",
    "            train_loss += loss\n",
    "            \n",
    "            model.backward(y)\n",
    "            model.apply_gradients(learning_rate, regularization_rate, momentum)\n",
    "            \n",
    "            choice = predict(y_pred)\n",
    "            train_accuracy += 1 if choice == np.argmax(y) else 0\n",
    "            \n",
    "        if batch % 400 == 0:\n",
    "            print(f'Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples')\n",
    "            \n",
    "    train_loss /= len(train_data)\n",
    "    train_accuracy /= len(train_data)\n",
    "    \n",
    "    loss_values.append(train_loss)\n",
    "    accuracy_values.append(train_accuracy)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss}, Train Accuracy: {train_accuracy}')        "
   ],
   "id": "64f58d30fe303869",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9319ca88ce6940429137a4197d69f5b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "--------------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 25600/60000 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[0;32m     25\u001B[0m model\u001B[38;5;241m.\u001B[39mbackward(y)\n\u001B[1;32m---> 26\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregularization_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m choice \u001B[38;5;241m=\u001B[39m predict(y_pred)\n\u001B[0;32m     29\u001B[0m train_accuracy \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m choice \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(y) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\repos-github\\ann-digit-recognition\\neural_network\\neural_network.py:90\u001B[0m, in \u001B[0;36mNeuralNetwork.apply_gradients\u001B[1;34m(self, learning_rate, regularization_rate, momentum)\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Start from the second layer and go until the last layer\u001B[39;00m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 90\u001B[0m     \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregularization_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\repos-github\\ann-digit-recognition\\neural_network\\layer.py:110\u001B[0m, in \u001B[0;36mLayer.apply_gradients\u001B[1;34m(self, learning_rate, regularization_rate, momentum)\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):  \u001B[38;5;66;03m# Iterate over columns\u001B[39;00m\n\u001B[0;32m    108\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights_velocities[i][j] \u001B[38;5;241m=\u001B[39m momentum \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights_velocities[i][j] \u001B[38;5;241m-\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m \\\n\u001B[0;32m    109\u001B[0m                                         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights_gradients[i][j]\n\u001B[1;32m--> 110\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i][j] \u001B[38;5;241m=\u001B[39m weight_decay \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i][j] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights_velocities[i][j]\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_neuron_out):\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases_velocities[i] \u001B[38;5;241m=\u001B[39m momentum \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases_velocities[i] \u001B[38;5;241m-\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases_gradients[i]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T21:36:51.021146Z",
     "start_time": "2024-05-06T21:36:51.021146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].plot(loss_values)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "\n",
    "ax[1].plot(accuracy_values)\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1d16a21c4a0a8d5e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
